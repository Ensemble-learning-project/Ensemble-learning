{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUOrAvk58ySa"
   },
   "source": [
    "# Decision Tree and Random Forest from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "E2NfR9qL23AX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from abc import ABC, abstractmethod\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    mean_squared_error\n",
    ")\n",
    "\n",
    "# TreeNode class represents a single node in the decision tree\n",
    "class TreeNode:\n",
    "    def __init__(self):\n",
    "        self.split_value = None\n",
    "        self.split_feature = None\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.leaf_value = None\n",
    "\n",
    "    def set_params(self, split_value, split_feature):\n",
    "        self.split_value = split_value\n",
    "        self.split_feature = split_feature\n",
    "\n",
    "    def set_children(self, left, right):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "# DecisionTree is an abstract base class for both the classifier and regressor\n",
    "class DecisionTree(ABC):\n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "\n",
    "    # Abstract methods for impurity and leaf value calculations\n",
    "    @abstractmethod\n",
    "    def _impurity(self, D):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def _leaf_value(self, D):\n",
    "        pass\n",
    "\n",
    "    # Recursive function to grow the decision tree\n",
    "    def _grow(self, node, D, level):\n",
    "        depth = (self.max_depth is None) or (self.max_depth >= (level + 1))\n",
    "        msamp = (self.min_samples_split <= D.shape[0])\n",
    "        n_cls = np.unique(D[:, -1]).shape[0] != 1\n",
    "\n",
    "        if depth and msamp and n_cls:\n",
    "            min_impurity = float('inf')\n",
    "            best_feature = None\n",
    "            best_split = None\n",
    "            left_D = None\n",
    "            right_D = None\n",
    "\n",
    "            for f in range(D.shape[1] - 1):\n",
    "                for s in np.unique(D[:, f]):\n",
    "                    D_l = D[D[:, f] <= s]\n",
    "                    D_r = D[D[:, f] > s]\n",
    "\n",
    "                    if D_l.size and D_r.size:\n",
    "                        impurity = (D_l.shape[0] / D.shape[0]) * self._impurity(D_l) + (D_r.shape[0] / D.shape[0]) * self._impurity(D_r)\n",
    "                        if impurity < min_impurity:\n",
    "                            min_impurity = impurity\n",
    "                            best_feature = f\n",
    "                            best_split = s\n",
    "                            left_D = D_l\n",
    "                            right_D = D_r\n",
    "\n",
    "            node.set_params(best_split, best_feature)\n",
    "            left_node = TreeNode()\n",
    "            right_node = TreeNode()\n",
    "            node.set_children(left_node, right_node)\n",
    "\n",
    "            self._grow(node.left, left_D, level + 1)\n",
    "            self._grow(node.right, right_D, level + 1)\n",
    "        else:\n",
    "            node.leaf_value = self._leaf_value(D)\n",
    "\n",
    "    # Recursive function to traverse the decision tree and predict the output\n",
    "    def _traverse(self, node, X_row):\n",
    "        if node.leaf_value is None:\n",
    "            split_value, split_feature = node.split_value, node.split_feature\n",
    "            if X_row[split_feature] <= split_value:\n",
    "                return self._traverse(node.left, X_row)\n",
    "            else:\n",
    "                return self._traverse(node.right, X_row)\n",
    "        else:\n",
    "            return node.leaf_value\n",
    "\n",
    "    # Train the decision tree using input data X and target labels y\n",
    "    def train(self, X, y):\n",
    "        D = np.concatenate((X, y.reshape(-1, 1)), axis=1)\n",
    "        self.root = TreeNode()\n",
    "        self._grow(self.root, D, 1)\n",
    "\n",
    "    # Predict the output for input data X using the trained decision tree\n",
    "    def predict(self, X):\n",
    "        return np.array([self._traverse(self.root, X_row) for X_row in X]).flatten()\n",
    "\n",
    "\n",
    "# DecisionTreeClassifier inherits from the DecisionTree class\n",
    "class DecisionTreeClassifier(DecisionTree):\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='gini'):\n",
    "        super().__init__(max_depth, min_samples_split)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def _gini(self, D):\n",
    "        gini = 0\n",
    "        for c in np.unique(D[:, -1]):\n",
    "            p = D[D[:, -1] == c].shape[0] / D.shape[0]\n",
    "            gini += p * (1 - p)\n",
    "        return gini\n",
    "\n",
    "    def _entropy(self, D):\n",
    "        entropy = 0\n",
    "        for c in np.unique(D[:, -1]):\n",
    "            p = D[D[:, -1] == c].shape[0] / D.shape[0]\n",
    "            entropy -= p * np.log2(p)\n",
    "        return entropy\n",
    "\n",
    "    def _impurity(self, D):\n",
    "        if self.criterion == 'gini':\n",
    "            return self._gini(D)\n",
    "        elif self.criterion == 'entropy':\n",
    "            return self._entropy(D)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion specified. Choose either 'gini' or 'entropy'.\")\n",
    "\n",
    "    def _leaf_value(self, D):\n",
    "        return stats.mode(D[:, -1], keepdims=False)[0]\n",
    "\n",
    "\n",
    "# DecisionTreeRegressor inherits from the DecisionTree class\n",
    "class DecisionTreeRegressor(DecisionTree):\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, criterion='mse'):\n",
    "        super().__init__(max_depth, min_samples_split)\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def _mse(self, D):\n",
    "        y_mean = np.mean(D[:, -1])\n",
    "        return np.sum((D[:, -1] - y_mean) ** 2) / D.shape[0]\n",
    "\n",
    "    def _mae(self, D):\n",
    "        y_mean = np.mean(D[:, -1])\n",
    "        return np.sum(np.abs(D[:, -1] - y_mean)) / D.shape[0]\n",
    "\n",
    "    def _impurity(self, D):\n",
    "        if self.criterion == 'mse':\n",
    "            return self._mse(D)\n",
    "        elif self.criterion == 'mae':\n",
    "            return self._mae(D)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid criterion specified. Choose either 'mse' or 'mae'.\")\n",
    "\n",
    "    def _leaf_value(self, D):\n",
    "        return np.mean(D[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FOkBdCj0afkp",
    "outputId": "b4d14ee3-d65c-4310-fac8-bb8c28c8520d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy: 0.9240\n",
      "Mean Squared Error: 0.7361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Classification example with the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "classifier = DecisionTreeClassifier(max_depth=3, min_samples_split=2, criterion='gini')\n",
    "classifier.train(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Regression example with the California Housing dataset\n",
    "housing_data = fetch_california_housing()\n",
    "X, y = housing_data.data, housing_data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "regressor = DecisionTreeRegressor(max_depth=3, min_samples_split=2, criterion='mse')\n",
    "regressor.train(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnDyDJis9fwS"
   },
   "source": [
    "In this example, we first load the \"Breast Cancer\" and \"California Housing\" datasets from scikit-learn. Then we split the data into training and test sets using the train_test_split function.\n",
    "\n",
    "For classification, we use the DecisionTreeClassifier class with a maximum depth of 3 and the Gini impurity criterion. We train the classifier on the training set and make predictions on the test set. Finally, we calculate the accuracy of the classification.\n",
    "\n",
    "For the regression, we use the DecisionTreeRegressor class with a maximum depth of 3 and the MSE impurity criterion. We train the regressor on the training set and make predictions on the test set. Finally, we calculate the mean squared error (MSE) to assess the performance of the regression.\n",
    "\n",
    "We used our decision tree implementation to perform classification with the Breast Cancer dataset and regression with the California Housing dataset. The results are as follows:\n",
    "\n",
    "Classification accuracy: 0.9240\n",
    "\n",
    "Mean Squared Error (MSE) for regression: 0.7361.\n",
    "\n",
    "These results show that our decision tree implementation is able to perform classification and regression tasks on real datasets. However, performance may be lower than scikit-learn's decision tree models due to differences in implementation, special case handling, default settings, and tree stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mx0zbtQx8qPK",
    "outputId": "2341745f-700e-41ca-95fe-58337fea7fd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn DecisionTreeClassifier accuracy: 0.9591\n",
      "Scikit-learn DecisionTreeRegressor Mean Squared Error (MSE): 0.6325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as SK_DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor as SK_DecisionTreeRegressor\n",
    "\n",
    "# Classification example with the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "classifier = SK_DecisionTreeClassifier(max_depth=3, min_samples_split=2, criterion='gini')\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Scikit-learn DecisionTreeClassifier accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Regression example with the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "regressor = SK_DecisionTreeRegressor(max_depth=3, min_samples_split=2, criterion='squared_error')\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Scikit-learn DecisionTreeRegressor Mean Squared Error (MSE): {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2CzKRak9KfI"
   },
   "source": [
    "The performance differences between our decision tree implementation and that of scikit-learn can be explained by several reasons:\n",
    "\n",
    "Implementation: Scikit-learn is a highly optimized library and uses efficient algorithms to build decision trees. Our implementation is quite simple and might not be as optimized, which can lead to differences in the quality of the trees built and, therefore, in the performance of the model.\n",
    "\n",
    "Special case handling: Scikit-learn more robustly handles special cases and implementation details, such as balanced splits and stopping conditions. Our simplified implementation may not handle these cases as effectively, which may affect model performance.\n",
    "\n",
    "Default Settings: The default settings for decision tree models in scikit-learn may be different from the ones we used in our implementation. Differences in parameters, such as maximum depth, minimum number of samples for splitting, and impurity criterion, can affect model performance.\n",
    "\n",
    "Stability: Decision trees are sensitive to variations in training data. A small change in the training data can result in a very different decision tree. Scikit-learn can use techniques to make the decision tree more stable, which can improve model performance.\n",
    "\n",
    "These reasons partly explain why the performance of our decision tree implementation may be lower than that of scikit-learn. It is important to note that scikit-learn is a very popular and widely used machine learning library which is designed to be efficient and accurate. Therefore, scikit-learn models are expected to generally provide better performance than implementations made from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "4ypXhwH29t85"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer, fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from scipy.stats import mode\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "\n",
    "    def _bootstrap_sample(self, X, y):\n",
    "        n_samples = X.shape[0]\n",
    "        idxs = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "        return X[idxs], y[idxs]\n",
    "\n",
    "    def _fit_tree(self, tree, X, y):\n",
    "        if self.max_features is None:\n",
    "            max_features = int(np.sqrt(X.shape[1]))\n",
    "        elif self.max_features == \"sqrt\":\n",
    "            max_features = int(np.sqrt(X.shape[1]))\n",
    "        elif self.max_features == \"log2\":\n",
    "            max_features = int(np.log2(X.shape[1]))\n",
    "        elif isinstance(self.max_features, int):\n",
    "            max_features = self.max_features\n",
    "        else:\n",
    "            raise ValueError(\"Invalid value for max_features.\")\n",
    "\n",
    "        features = np.random.choice(X.shape[1], size=max_features, replace=False)\n",
    "        tree.fit(X[:, features], y)\n",
    "        return tree, features\n",
    "\n",
    "    def train(self, X, y, tree_class):\n",
    "        self.trees = []\n",
    "        self.tree_features = []\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            tree = tree_class(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            X_sample, y_sample = self._bootstrap_sample(X, y)\n",
    "            tree, features = self._fit_tree(tree, X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "            self.tree_features.append(features)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "\n",
    "        for tree, features in zip(self.trees, self.tree_features):\n",
    "            predictions.append(tree.predict(X[:, features]))\n",
    "\n",
    "        return mode(predictions, axis=0, keepdims=False)[0].flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80VHwDaoh7hF",
    "outputId": "ef8e1db8-f4bf-421f-fd83-2d1b7ac46037"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Classifier accuracy: 0.9708\n",
      "RandomForest Regressor Mean Squared Error (MSE): 0.6877\n",
      "RandomForest Regressor R-squared (R^2): 0.4760\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# RandomForestClassifier example with the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_classifier = RandomForest(n_estimators=100, max_depth=3, min_samples_split=2, max_features=\"sqrt\")\n",
    "rf_classifier.train(X_train, y_train, DecisionTreeClassifier)\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Classifier accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# RandomForestRegressor example with the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_regressor = RandomForest(n_estimators=100, max_depth=3, min_samples_split=2, max_features=\"sqrt\")\n",
    "rf_regressor.train(X_train, y_train, DecisionTreeRegressor)\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Random Forest Regressor Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Random Forest Regressor R-squared (R^2): {r2:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PeSr4dsPKOi"
   },
   "source": [
    "In this example, we used the RandomForest class to perform both classification and regression. For classification, we used the Breast Cancer dataset, and for regression, we used the California Housing dataset. Results are displayed as accuracy for classification and mean squared error (MSE) for regression.\n",
    "\n",
    "This implementation of Random Forest is based on the implementation of DecisionTreeClassifier and DecisionTreeRegressor that we implemented previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
